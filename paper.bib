@misc{Amazon2011,
abstract = {Amazon EC2s simple web service interface allows you to obtain and configure capacity with minimal friction.},
archivePrefix = {arXiv},
arxivId = {arXiv:1105.1408v1},
author = {Amazon},
booktitle = {Amazon Web Services LLC},
eprint = {arXiv:1105.1408v1},
title = {{Amazon Elastic Compute Cloud (Amazon EC2)}},
url = {http://aws.amazon.com/ec2/},
volume = {2010},
year = {2011}
}
@inproceedings{baroni-dinu-kruszewski:2014:P14-1,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.3115/v1/P14-1023},
file = {:home/egon/Books/mndlyBooks/Baroni, Dinu, Kruszewski/Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1 Long Papers)/Baroni, Dinu, Kruszewski - 2014 - Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vect.pdf:pdf},
isbn = {9781937284725},
pages = {238--247},
publisher = {Association for Computational Linguistics},
title = {{Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors}},
url = {http://www.aclweb.org/anthology/P14-1023},
year = {2014}
}
@article{Bastien2012a,
abstract = {Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5590v1},
author = {Bastien, F and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian and Bergeron, Arnaud and Bouchard, Nicolas and Warde-Farley, David and Bengio, Yoshua},
eprint = {arXiv:1211.5590v1},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--10},
title = {{Theano: new features and speed improvements}},
url = {http://arxiv.org/abs/1211.5590},
year = {2012}
}
@article{Benello1989,
abstract = {A 560-unit neural network with two layers of modifiable connections was trained by means of back-propagation to disambiguate the syntactic categories of words in samples of text taken from the Brown Corpus. After training, the network was able to successfully disambiguate words in previously unanalyzed text with 95{\%} accuracy, a performance level comparable to the best current computational techniques for the disambiguation of syntactic function. The model incorporates plausible psychological constraints on its input and output representations, and exhibited human-like behavior during parts of the learning process. The network's success suggests that syntactic category disambiguation may be mainly a low-level, bottom-up process with little dependence on the recognition of higher-level syntactic structures. Although the network simulates only a restricted component of the human language processing mechanism, its intrinsic ability to use partially formed data should allow it to be easily integrated into a full-scale language comprehension system. The model's overall performance level, along with its psychological plausibility, indicates that neural networks may be a new and useful approach to building human language processing models.},
author = {Benello, Julian and Mackie, Andrew W. and Anderson, James A.},
doi = {10.1016/0885-2308(89)90018-1},
file = {:home/egon/Books/mndlyBooks/Benello, Mackie, Anderson/Computer Speech {\&} Language/Benello, Mackie, Anderson - 1989 - Syntactic category disambiguation with neural networks.pdf:pdf},
issn = {08852308},
journal = {Computer Speech {\&} Language},
month = {jul},
number = {3},
pages = {203--217},
title = {{Syntactic category disambiguation with neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/0885230889900181},
volume = {3},
year = {1989}
}
@article{Bergstra2010a,
abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy's syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy's syntax and semantics, while being statically typed and functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.},
author = {Bergstra, James and Breuleux, Olivier and Bastien, Frederic Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
journal = {Proceedings of the Python for Scientific Computing Conference (SciPy)},
number = {Scipy},
pages = {1--7},
title = {{Theano: a CPU and GPU math compiler in Python}},
url = {http://www-etud.iro.umontreal.ca/{~}wardefar/publications/theano{\_}scipy2010.pdf},
year = {2010}
}
@article{Chen2015,
abstract = {Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.},
archivePrefix = {arXiv},
arxivId = {1512.04906},
author = {Chen, Welin and Grangier, David and Auli, Michael},
eprint = {1512.04906},
file = {:home/egon/Books/mndlyBooks/Chen, Grangier, Auli/Unknown/Chen, Grangier, Auli - 2015 - Strategies for Training Large Vocabulary Neural Language Models.pdf:pdf},
isbn = {9781467303675},
pages = {12},
title = {{Strategies for Training Large Vocabulary Neural Language Models}},
url = {http://arxiv.org/abs/1512.04906},
year = {2015}
}
@article{Chetlur2014a,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36{\%} on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff},
eprint = {1410.0759},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@inproceedings{Habernal.et.al.2016.LREC,
abstract = {Large Web corpora containing full documents with permissive licenses are crucial for many NLP tasks. In this article we present the construction of 12 million-pages Web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs. Our highly-scalable Hadoop-based framework is able to process the full CommonCrawl corpus on 2000+ CPU cluster on the Amazon Elastic Map/Reduce infrastructure. The processing pipeline includes license identification, state-of-the-art boilerplate removal, exact duplicate and near-duplicate document removal, and language detection. The construction of the corpus is highly configurable and fully reproducible, and we provide both the framework (DKPro C4CorpusTools) and the resulting data (C4Corpus) to the research community.},
address = {Portoro{\v{z}}, Slovenia},
author = {Habernal, Ivan and Zayed, Omnia and Gurevych, Iryna},
booktitle = {Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)},
file = {:home/egon/Books/mndlyBooks/Habernal, Zayed, Gurevych/Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)/Habernal, Zayed, Gurevych - 2016 - C4Corpus Multilingual Web-size corpus with free license.pdf:pdf},
publisher = {European Language Resources Association (ELRA)},
title = {{C4Corpus: Multilingual Web-size corpus with free license}},
  month     = {May},
year = {2016},
  pages     = {(to appear)},
  url       = {TBA}
}
@article{Jozefowicz2016,
abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
archivePrefix = {arXiv},
arxivId = {1602.02410},
author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
eprint = {1602.02410},
file = {:home/egon/Books/mndlyBooks/Jozefowicz et al/Unknown/Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling.pdf:pdf},
title = {{Exploring the Limits of Language Modeling}},
url = {http://arxiv.org/abs/1602.02410},
year = {2016}
}
@article{Karimi2015,
author = {Karimi, Sarvnaz and Yin, Jie and Baum, Jiri},
doi = {10.1162/COLI_a_00230},
file = {:home/egon/Books/mndlyBooks/Karimi, Yin, Baum/Computational Linguistics/Karimi, Yin, Baum - 2015 - Evaluation Methods for Statistically Dependent Text.pdf:pdf},
journal = {Computational Linguistics},
month = {sep},
number = {3},
pages = {539--548},
title = {{Evaluation Methods for Statistically Dependent Text}},
url = {http://www.mitpressjournals.org/doi/10.1162/COLI{\_}a{\_}00230},
volume = {41},
year = {2015}
}
@article{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
eprint = {1508.06615},
file = {:home/egon/Books/mndlyBooks/Kim et al/Aaai/Kim et al. - 2016 - Character-Aware Neural Language Models.pdf:pdf},
journal = {Aaai},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@book{whyweposthtwc2016,
author = {Miller, Daniel and Costa, Elisabetta and Haynes, Nell and McDonald, Tom and Nicolescu, Razvan and Sinanan, Jolyanna and Spyer, Juliano and Venkatraman, Shriram and Wang, Xinyuan},
doi = {10.14324/111.9781910634493},
edition = {Why We Pos},
file = {:home/egon/Books/mndlyBooks/Miller et al/Unknown/Miller et al. - 2016 - How the World Changed Social Media.pdf:pdf},
isbn = {9781910634493},
month = {feb},
publisher = {UCL Press},
title = {{How the World Changed Social Media}},
url = {http://discovery.ucl.ac.uk/1474805/1/How-the-World-Changed-Social-Media.pdf http://discovery.ucl.ac.uk/1474805/},
year = {2016}
}
@inproceedings{santos2014learning,
abstract = {Distributed word representations have recently been proven to be an invaluable resource for NLP. These representations are normally learned using neural networks and capture syntactic and semantic information about words. Informa-tion about word morphology and shape is nor-mally ignored when learning word representa-tions. However, for tasks like part-of-speech tag-ging, intra-word information is extremely use-ful, specially when dealing with morphologically rich languages. In this paper, we propose a deep neural network that learns character-level repre-sentation of words and associate them with usual word representations to perform POS tagging. Using the proposed approach, while avoiding the use of any handcrafted feature, we produce state-of-the-art POS taggers for two languages: En-glish, with 97.32{\%} accuracy on the Penn Tree-bank WSJ corpus; and Portuguese, with 97.47{\%} accuracy on the Mac-Morpho corpus, where the latter represents an error reduction of 12.2{\%} on the best previous known result.},
author = {{Nogueira dos Santos}, C{\'{i}}cero and Zadrozny, Bianca},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
file = {:home/egon/Books/mndlyBooks/Nogueira dos Santos, Zadrozny/Proceedings of the 31st International Conference on Machine Learning (ICML-14)/Nogueira dos Santos, Zadrozny - 2014 - Learning Character-level Representations for Part-of-Speech Tagging.pdf:pdf},
pages = {1818--1826},
title = {{Learning Character-level Representations for Part-of-Speech Tagging}},
year = {2014}
}
@inproceedings{Samardzic2015,
abstract = {Swiss dialects of German are, unlike most dialects of well standardised languages, widely used in everyday communication. Despite this fact, they lack tools and resources for natural language processing. The main reason for this is the fact that the dialects are mostly spoken and that written resources are small and highly inconsistent. This paper addresses the great variability in writing that poses a problem for automatic processing. We propose an automatic approach to normalising the variants to a single representation intended for processing tools' internal use (not shown to human users). We manually create a sample of transcribed and normalised texts, which we use to train and test three methods based on machine translation: word-by-word mappings, character-based machine translation, and language modelling. We show that an optimal combination of the three approaches gives better results than any of them separately.},
author = {Samard{\v{z}}i{\'{c}}, Tanja and Scherrer, Yves and Glaser, Elvira},
booktitle = {Proceedings of the 7th Language and Technology Conference},
file = {:home/egon/Books/mndlyBooks/Samard{\v{z}}i{\'{c}}, Scherrer, Glaser/Proceedings of the 7th Language and Technology Conference/Samard{\v{z}}i{\'{c}}, Scherrer, Glaser - 2015 - Normalising orthographic and dialectal variants for the automatic processing of Swiss German.pdf:pdf},
title = {{Normalising orthographic and dialectal variants for the automatic processing of Swiss German}},
url = {http://archive-ouverte.unige.ch/unige:82397},
year = {2015}
}
@article{Sundermeyer2012,
abstract = {Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a se- quence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of re- current models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we ana- lyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 {\%} relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.},
author = {Sundermeyer, Martin and Schl, Ralf and Ney, Hermann},
file = {:home/egon/Books/mndlyBooks/Sundermeyer, Schl, Ney/Proc. Interspeech/Sundermeyer, Schl, Ney - 2012 - LSTM Neural Networks for Language Modeling.pdf:pdf},
isbn = {9781622767595},
journal = {Proc. Interspeech},
keywords = {LSTM neural networks,language modeling,recurrent neural networks},
title = {{LSTM Neural Networks for Language Modeling}},
year = {2012}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {1409.3215},
file = {:home/egon/Books/mndlyBooks/Sutskever, Vinyals, Le/Advances in Neural Information Processing Systems (NIPS)/Sutskever, Vinyals, Le - 2014 - Sequence to sequence learning with neural networks.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@phdthesis{Weber2015,
abstract = {In dieser Arbeit werden italienische Tweets hinsichtlich des nicht-standardisierten Textes in den Tweets normalisiert. Als Ausgangspunkt werden Methoden und Verfahren besonders aus der englischen Textnormalisierung verwendet und analysiert, ob diese Methoden ebenfalls im Italienischen zu guten Ergebnissen f{\"{u}}hren. Das Hauptaugenmerk liegt auf der Normalisierung von Akronymen innerhalb der Tweets, jedoch werden auch andere Ph{\"{a}}nomene, die bei der Verwendung eines Chat-Jargons auftreten, welche das Verarbeiten der Tweets mit nat{\"{u}}rlich sprachlichen Programmen erschwert, normalisiert.},
author = {Weber, Daniel Julian},
file = {:home/egon/Books/mndlyBooks/Weber/Unknown/Weber - 2015 - Text Normalisierung f{\"{u}}r Italienische Twitter-Microblogs.pdf:pdf},
month = {nov},
number = {November},
school = {Ludwig-Maxmilians-Universit{\"{a}}t M{\"{u}}nchen},
title = {{Text Normalisierung f{\"{u}}r Italienische Twitter-Microblogs}},
type = {bachelorsthesis},
url = {http://www.zhekova.net/Teaching{\_}Supervision{\_}files/Daniel{\_}Weber{\_}BSc.pdf},
year = {2015}
}
@inproceedings{Yang2013,
address = {Seattle, Washington, USA},
author = {Yang, Yi and Eisenstein, Jacob},
booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
file = {:home/egon/Books/mndlyBooks/Yang, Eisenstein/Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing/Yang, Eisenstein - 2013 - A Log-Linear Model for Unsupervised Text Normalization.pdf:pdf},
isbn = {9781937284978},
number = {October},
pages = {61--72},
publisher = {Association for Computational Linguistics},
title = {{A Log-Linear Model for Unsupervised Text Normalization}},
year = {2013}
}
@article{DBLP:journals/corr/abs-1301-3781,
  author    = {Tomas Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  journal   = {CoRR},
  volume    = {abs/1301.3781},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Thu, 07 May 2015 20:02:01 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1301-3781},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{arXiv:1310.4546,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1310.4546},
file = {:home/egon/Books/mndlyBooks/Mikolov et al/Unknown/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
month = {oct},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}
@article{Theano2016,
abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
archivePrefix = {arXiv},
arxivId = {1605.02688},
author = {{The Theano Development Team} and Rami Al{-}Rfou and Guillaume Alain and
Amjad Almahairi and et al.},
eprint = {1605.02688},
file = {:home/egon/Books/mndlyBooks/{\{}The Theano Development Team{\}}/Unknown/{\{}The Theano Development Team{\}} - 2016 - Theano A Python framework for fast computation of mathematical expressions.pdf:pdf},
title = {{Theano: {A} Python framework for fast computation of mathematical expressions}},
url = {http://arxiv.org/abs/1605.02688},
year = {2016},
journal   = {CoRR},
volume    = {abs/1605.02688}
}
@misc{chollet2015,
author = {Chollet, François},
title = {{Keras: Deep Learning library for Theano and TensorFlow}},
year = {2015},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/fchollet/keras}},
urldate = {2016-03-22},
commit = {657b9fb48e93b59083d2e0b8a5e4daf237179dbc}
}
@article{Giesbrecht2009,
    author = {Giesbrecht, Eugenie and Evert, Stefan},
    file = {:home/egon/Books/mndlyBooks/Giesbrecht, Evert/Web as Corpus Workshop (WAC5)/Giesbrecht, Evert - 2009 - Is Part-of-Speech Tagging a Solved Task An Evaluation of POS Taggers for the German Web as Corpus.pdf:pdf},
    journal = {Web as Corpus Workshop (WAC5)},
    title = {{Is Part-of-Speech Tagging a Solved Task? An Evaluation of POS Taggers for the German Web as Corpus}},
    url = {http://sigwac.org.uk/raw-attachment/wiki/WAC5/WAC5{\_}proceedings.pdf{\#}page=27},
    year = {2009}
}
@inproceedings{empirist2016,
    address = {Berlin, Germany},
    author = {Bei{\ss}wenger, Michael and Bartsch, Sabine and Evert, Stefan and W{\"{u}}rzner, Kay-Michael},
    booktitle = {Proceedings of the 10th Web as Corpus Workshop (WAC-X)},
    title = {{EmpiriST 2015: A Shared Task on the Automatic Linguistic Annotation of Computer-Mediated Communication, Social Media and Web Corpora}},
    year = {2016}
}
@Article{Brants2004,
    author="Brants, Sabine
        and Dipper, Stefanie
        and Eisenberg, Peter
        and Hansen-Schirra, Silvia
        and K{\"o}nig, Esther
        and Lezius, Wolfgang
        and Rohrer, Christian
        and Smith, George
        and Uszkoreit, Hans",
    title="TIGER: Linguistic Interpretation of a German Corpus",
    journal="Research on Language and Computation",
    year="2004",
    volume="2",
    number="4",
    pages="597--620",
    abstract="This paper reports on the TIGER Treebank, a corpus of currently 40,000 syntactically annotated German newspaper sentences. We describe what kind of information is encoded in the treebank and introduce the different representation formats that are used for the annotation and exploitation of the treebank. We explain the different methods used for the annotation: interactive annotation, using the tool ANNOTATE, and LFG parsing. Furthermore, we give an account of the annotation scheme used for the TIGER treebank. This scheme is an extended and improved version of the NEGRA annotation scheme and we illustrate in detail the linguistic extensions that were made concerning the annotation in the TIGER project. The main differences are concerned with coordination, verb-subcategorization, expletives as well as proper nouns. In addition, the paper also presents the query tool TIGERSearch that was developed in the project to exploit the treebank in an adequate way. We describe the query language which was designed to facilitate a simple formulation of complex queries; furthermore, we shortly introduce TIGER in, a graphical user interface for query input. The paper concludes with a summary and some directions for future work.",
    issn="1572-8706",
    doi="10.1007/s11168-004-7431-3",
    url="http://dx.doi.org/10.1007/s11168-004-7431-3"
}
@INPROCEEDINGS{SchaeferBildhauer2012,
    author = {Roland Sch{\"{a}}fer and Felix Bildhauer},
    title = {Building Large Corpora from the Web Using a New Efficient
             Tool Chain},
    year = {2012},
    pages = {486--493},
    crossref = {LREC2012}
}

@PROCEEDINGS{LREC2012,
    title = {Proceedings of the Eighth International Conference on Language
             Resources and Evaluation (LREC'12)},
    year = {2012},
    editor = {Nicoletta Calzolari and Khalid Choukri and Thierry Declerck
              and Mehmet U\u{g}ur Do\u{g}an and Bente Maegaard and Joseph
              Mariani and Jan Odijk and Stelios Piperidis},
    address = {Istanbul},
    publisher = {ELRA},
    booktitle = {Proceedings of the Eight International Conference
                 on Language Resources and Evaluation (LREC'12)}
}
@article{TACL570,
    author = {Levy, Omer  and Goldberg, Yoav  and Dagan, Ido },
    title = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {3},
    year = {2015},
    keywords = {},
    abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
    issn = {2307-387X},
    url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
    pages = {211--225}
}
@article{Hochreiter:1997:LSM:1246443.1246450,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@inproceedings{1120431,
   author = {Jakub{\'\i}{\v{c}}ek, Milo{\v{s}} and Kilgarriff, Adam and Kov{\'a}{\v{r}}, Vojt{\v{e}}ch and Rychl{\`y}, Pavel and Suchomel, V{\'\i}t},
   address = {Lancaster},
   booktitle = {7th International Corpus Linguistics Conference CL 2013},
   howpublished = {online},
   location = {Lancaster},
   pages = {125-127},
   title = {The {TenTen} Corpus Family},
   url = {http://ucrel.lancs.ac.uk/cl2013/},
   year = {2013}
}
@article{Beisswenger2013,
    author = {Bei{\ss}wenger, Michael},
    file = {:home/egon/Books/mndlyBooks/Bei{\ss}wenger/LINSE - Linguistik Server Essen/Bei{\ss}wenger - 2013 - Das Dortmunder Chat-Korpus ein annotiertes Korpus zur Sprachverwendung und sprachlichen Variation in der deutschspr.pdf:pdf},
    journal = {LINSE - Linguistik Server Essen},
    pages = {1--13},
    title = {{Das Dortmunder Chat-Korpus: ein annotiertes Korpus zur Sprachverwendung und sprachlichen Variation in der deutschsprachigen Chat-Kommunikation}},
    url = {http://www.linse.uni-due.de/tl{\_}files/PDFs/Publikationen-Rezensionen/Chatkorpus{\_}Beisswenger{\_}2013.pdf},
    year = {2013}
}
@inproceedings{I13-1041,
    address = {Nagoya, Japan},
    author = {Baldwin, Timothy and Cook, Paul and Lui, Marco and MacKinlay, Andrew and Wang, Li},
    booktitle = {Proceedings of the Sixth International Joint Conference on Natural Language Processing},
    file = {:home/egon/Books/mndlyBooks/Baldwin et al/Proceedings of the Sixth International Joint Conference on Natural Language Processing/Baldwin et al. - 2013 - How Noisy Social Media Text, How Diffrnt Social Media Sources.pdf:pdf},
    number = {October},
    pages = {356--364},
    publisher = {Asian Federation of Natural Language Processing},
    title = {{How Noisy Social Media Text, How Diffrnt Social Media Sources?}},
    url = {http://aclweb.org/anthology/I13-1041},
    year = {2013}
}
@article{Androutsopoulos2007,
author = {Androutsopoulos, Jannis K.},
file = {:home/egon/Books/mndlyBooks/Androutsopoulos/Mitteilungen des Deutschen Germanistenverbandes/Androutsopoulos - 2007 - Neue Medien – neue Schriftlichkeit.pdf:pdf},
journal = {Mitteilungen des Deutschen Germanistenverbandes},
pages = {72--97},
title = {{Neue Medien – neue Schriftlichkeit?}},
volume = {1},
year = {2007}
}
@book{Crystal2001,
address = {Cambridge},
author = {Crystal, David},
doi = {10.1017/CBO9781139164771},
isbn = {9781139164771},
publisher = {Cambridge University Press},
title = {{Language and the Internet}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139164771},
year = {2001}
}
